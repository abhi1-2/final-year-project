{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "main_path='UTKface_Aligned_cropped/crop_part1/'\n",
    "d=os.listdir(main_path)\n",
    "count=0\n",
    "dict={}\n",
    "for i in range(1,120):\n",
    "    dict[i]=[]\n",
    "\n",
    "\n",
    "for i,dirs in enumerate(sorted(d)):\n",
    "    age=dirs.split(\"_\")[0]\n",
    "    dict[int(age)].append(dirs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "age_grp=['1-10','11-20','21-30','31-40','41-50','51-60']\n",
    "main_dict={}\n",
    "for key in age_grp:\n",
    "    main_dict[key]=[]\n",
    "for key in age_grp:\n",
    "    k=key.split(\"-\")\n",
    "    for i in range(int(k[0]),int(k[1])+1):\n",
    "        count=0\n",
    "        main_dict[key].append(dict[i][0:25])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in main_dict.keys():\n",
    "    arr=[]\n",
    "    for el in main_dict[k]:\n",
    "        for item in el:\n",
    "            arr.append(item)\n",
    "    main_dict[k]=arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "for key in main_dict.keys():\n",
    "    for pic in main_dict[key]:\n",
    "        img=cv.imread(main_path+pic)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_dict={}\n",
    "count=0\n",
    "for key in main_dict.keys():\n",
    "    count=0\n",
    "    for pic in main_dict[key]:\n",
    "        count+=1\n",
    "    num_dict[key]=count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1-10': 250,\n",
       " '11-20': 250,\n",
       " '21-30': 250,\n",
       " '31-40': 250,\n",
       " '41-50': 250,\n",
       " '51-60': 250}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avisheksarkar/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/avisheksarkar/Desktop/final_year_project/UTKface_Aligned_cropped/crop_part1/'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import dlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "predictor_model=\"shape_predictor_68_face_landmarks.dat\"\n",
    "predictor = dlib.shape_predictor(predictor_model)\n",
    "facerec=dlib.face_recognition_model_v1('dlib_face_recognition_resnet_model_v1.dat')\n",
    "d=os.listdir(main_path)\n",
    "main_path=os.getcwd()+'/UTKface_Aligned_cropped/crop_part1/'\n",
    "main_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-10\n",
      "11-20\n",
      "21-30\n",
      "31-40\n",
      "41-50\n",
      "51-60\n",
      "(1500, 128)\n",
      "(1500, 6)\n"
     ]
    }
   ],
   "source": [
    "img_array=[]\n",
    "img_labels=[]\n",
    "import cv2 as cv\n",
    "for i,key in enumerate(main_dict.keys()):\n",
    "    print(key)\n",
    "    label=[0,0,0,0,0,0]\n",
    "    for pic in main_dict[key]:\n",
    "        \n",
    "        img=cv.imread(main_path+pic)\n",
    "        gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "        face_rect= face_detector(gray, 1)\n",
    "        for k,d in enumerate(face_rect):\n",
    "            shape=predictor(gray,d)\n",
    "            face_descriptor = facerec.compute_face_descriptor(img,shape )\n",
    "                \n",
    "        label[i]=1\n",
    "        img_array.append(face_descriptor)\n",
    "        img_labels.append(label)\n",
    "img_dataset=np.array(img_array)\n",
    "img_labell=np.array(img_labels)\n",
    "print(img_dataset.shape)\n",
    "print(img_labell.shape)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "(trainX, testX, trainY, testY) = train_test_split(img_dataset,\n",
    "\timg_labell, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_labell[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_dim=128, activation='sigmoid'))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "#sgd = optimizers.SGD(lr=0.063, decay=0.9)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1275 samples, validate on 225 samples\n",
      "Epoch 1/15\n",
      "1275/1275 [==============================] - 1s - loss: 0.2730 - acc: 0.8685 - val_loss: 0.8075 - val_acc: 0.7496\n",
      "Epoch 2/15\n",
      "1275/1275 [==============================] - 1s - loss: 0.2702 - acc: 0.8684 - val_loss: 0.6852 - val_acc: 0.7807\n",
      "Epoch 3/15\n",
      "1275/1275 [==============================] - 1s - loss: 0.2688 - acc: 0.8680 - val_loss: 0.7761 - val_acc: 0.6770\n",
      "Epoch 4/15\n",
      "1275/1275 [==============================] - 1s - loss: 0.2627 - acc: 0.8733 - val_loss: 0.7475 - val_acc: 0.6926\n",
      "Epoch 5/15\n",
      "1275/1275 [==============================] - 1s - loss: 0.2617 - acc: 0.8724 - val_loss: 1.0263 - val_acc: 0.6763\n",
      "Epoch 6/15\n",
      "1275/1275 [==============================] - 1s - loss: 0.2656 - acc: 0.8733 - val_loss: 0.8051 - val_acc: 0.6956\n",
      "Epoch 7/15\n",
      "1275/1275 [==============================] - 1s - loss: 0.2637 - acc: 0.8705 - val_loss: 0.8042 - val_acc: 0.6741\n",
      "Epoch 8/15\n",
      "1275/1275 [==============================] - 1s - loss: 0.2653 - acc: 0.8682 - val_loss: 0.6875 - val_acc: 0.7326\n",
      "Epoch 9/15\n",
      "1275/1275 [==============================] - 1s - loss: 0.2655 - acc: 0.8705 - val_loss: 0.9297 - val_acc: 0.7133\n",
      "Epoch 10/15\n",
      "1275/1275 [==============================] - 1s - loss: 0.2611 - acc: 0.8722 - val_loss: 0.7936 - val_acc: 0.7000\n",
      "Epoch 11/15\n",
      "1275/1275 [==============================] - 1s - loss: 0.2566 - acc: 0.8765 - val_loss: 0.7739 - val_acc: 0.6807\n",
      "Epoch 12/15\n",
      "1275/1275 [==============================] - 1s - loss: 0.2590 - acc: 0.8761 - val_loss: 0.7050 - val_acc: 0.7111\n",
      "Epoch 13/15\n",
      "1275/1275 [==============================] - 1s - loss: 0.2535 - acc: 0.8793 - val_loss: 0.7493 - val_acc: 0.7096\n",
      "Epoch 14/15\n",
      "1275/1275 [==============================] - 1s - loss: 0.2574 - acc: 0.8757 - val_loss: 0.7200 - val_acc: 0.6867\n",
      "Epoch 15/15\n",
      "1275/1275 [==============================] - 1s - loss: 0.2497 - acc: 0.8792 - val_loss: 0.7217 - val_acc: 0.7230\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1300887f0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(img_dataset, img_labell, epochs=15, shuffle=True,batch_size=20,validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00180773 0.12759055 0.6461065  0.17555496 0.04380823 0.00513201]]\n",
      "[[6.4001841e-05 8.5844705e-03 2.3177977e-01 3.8947758e-01 3.2916719e-01\n",
      "  4.0926974e-02]]\n",
      "[[1.2491153e-04 3.2963637e-02 6.1677861e-01 2.6098248e-01 7.4529946e-02\n",
      "  1.4620399e-02]]\n",
      "[[5.7044988e-05 1.2668761e-02 3.6876744e-01 3.7720913e-01 2.0701981e-01\n",
      "  3.4277882e-02]]\n",
      "[[2.1563385e-04 3.4977317e-02 5.5354911e-01 2.9001933e-01 1.0511034e-01\n",
      "  1.6128249e-02]]\n",
      "[[1.8794394e-04 2.4457971e-02 3.9716104e-01 3.6549184e-01 1.8790442e-01\n",
      "  2.4796769e-02]]\n",
      "[[2.0665136e-04 3.0633226e-02 4.6130982e-01 3.3998719e-01 1.4685416e-01\n",
      "  2.1008966e-02]]\n",
      "[[1.2544464e-04 1.8121097e-02 3.6832556e-01 3.7189099e-01 2.1258932e-01\n",
      "  2.8947607e-02]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-8d7ef9d273b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#image=cv.imread(image)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdetected_faces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetected_faces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mface_rect\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetected_faces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alpha=['baby','teen','young','mid-age','old','very old']\n",
    "cap = cv.VideoCapture(0)\n",
    "ret,frame=cap.read()\n",
    "while ret:\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "#image=os.getcwd()+'/test_images/srija_test.jpeg'\n",
    "#image=cv.imread(image)\n",
    "#gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    detected_faces = face_detector(gray, 1)\n",
    "    if(detected_faces):\n",
    "        for i,face_rect in enumerate(detected_faces):\n",
    "            shape=predictor(gray,face_rect)\n",
    "            face_descriptor = facerec.compute_face_descriptor(frame,shape)\n",
    "            face_descriptor=np.array(face_descriptor)\n",
    "            face_descriptor=face_descriptor.reshape((-1,1*128)).astype(np.float32)\n",
    "            print(model.predict(face_descriptor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00087246 0.0554398  0.3229685  0.39640328 0.20473544 0.01958043]]\n"
     ]
    }
   ],
   "source": [
    "image=os.getcwd()+'/test_images/kid_1.jpg'\n",
    "image=cv.imread(image)\n",
    "gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "detected_faces = face_detector(gray, 1)\n",
    "if(detected_faces):\n",
    "    for i,face_rect in enumerate(detected_faces):\n",
    "        shape=predictor(gray,face_rect)\n",
    "        face_descriptor = facerec.compute_face_descriptor(frame,shape)\n",
    "        face_descriptor=np.array(face_descriptor)\n",
    "        face_descriptor=np.array(face_descriptor.reshape((-1,1*128)).astype(np.float32))\n",
    "        print(model.predict(face_descriptor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"models/model_age_1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"models/model_age_1.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194, 259, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
